#!/bin/bash

# Ollama Deployment Script for Taskego
# This script helps deploy Ollama to different environments

set -e

echo "üöÄ Taskego Ollama Deployment Script"
echo "=================================="

# Check if Docker is installed
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker is not installed. Please install Docker first."
    exit 1
fi

# Check if Docker Compose is installed
if ! command -v docker-compose &> /dev/null; then
    echo "‚ùå Docker Compose is not installed. Please install Docker Compose first."
    exit 1
fi

# Function to deploy locally
deploy_local() {
    echo "üì¶ Deploying Ollama locally..."
    
    # Build and start the container
    docker-compose up -d --build
    
    # Wait for Ollama to be ready
    echo "‚è≥ Waiting for Ollama to start..."
    sleep 10
    
    # Check if Ollama is running
    if curl -s http://localhost:11434/api/tags > /dev/null; then
        echo "‚úÖ Ollama is running successfully!"
        echo "üåê Access URL: http://localhost:11434"
        echo "üîß API Endpoint: http://localhost:11434/api"
    else
        echo "‚ùå Ollama failed to start. Check logs with: docker logs taskego-ollama"
        exit 1
    fi
}

# Function to deploy to cloud
deploy_cloud() {
    echo "‚òÅÔ∏è  Deploying Ollama to cloud..."
    
    # Check if REMOTE_HOST is set
    if [ -z "$REMOTE_HOST" ]; then
        echo "‚ùå Please set REMOTE_HOST environment variable"
        echo "Example: export REMOTE_HOST=user@your-server.com"
        exit 1
    fi
    
    echo "üì§ Copying files to $REMOTE_HOST..."
    scp -r . $REMOTE_HOST:~/ollama
    
    echo "üîß Setting up Ollama on remote server..."
    ssh $REMOTE_HOST << 'EOF'
        cd ~/ollama
        docker-compose up -d --build
        
        # Wait for Ollama to start
        sleep 15
        
        # Check if running
        if curl -s http://localhost:11434/api/tags > /dev/null; then
            echo "‚úÖ Ollama is running on remote server!"
        else
            echo "‚ùå Ollama failed to start on remote server"
            exit 1
        fi
    EOF
    
    echo "‚úÖ Ollama deployed to cloud successfully!"
    echo "üåê Remote URL: http://$REMOTE_HOST:11434"
}

# Function to pull models
pull_models() {
    echo "üì• Pulling Ollama models..."
    
    # Check if Ollama is running
    if ! curl -s http://localhost:11434/api/tags > /dev/null; then
        echo "‚ùå Ollama is not running. Start it first with: docker-compose up -d"
        exit 1
    fi
    
    echo "üì• Pulling llama2:7b (fast, good for basic responses)..."
    docker exec taskego-ollama ollama pull llama2:7b
    
    echo "üì• Pulling llama2:13b (balanced speed/quality, recommended)..."
    docker exec taskego-ollama ollama pull llama2:13b
    
    echo "‚úÖ Models pulled successfully!"
    echo "üìã Available models:"
    docker exec taskego-ollama ollama list
}

# Function to test Ollama
test_ollama() {
    echo "üß™ Testing Ollama..."
    
    if ! curl -s http://localhost:11434/api/tags > /dev/null; then
        echo "‚ùå Ollama is not running. Start it first with: docker-compose up -d"
        exit 1
    fi
    
    echo "üìù Testing basic response..."
    response=$(curl -s -X POST http://localhost:11434/api/generate \
        -H "Content-Type: application/json" \
        -d '{
            "model": "llama2:7b",
            "prompt": "Hello, can you help me with Taskego services?",
            "stream": false
        }')
    
    if echo "$response" | grep -q "response"; then
        echo "‚úÖ Ollama is responding correctly!"
        echo "üìù Response preview:"
        echo "$response" | jq -r '.response' | head -c 100
        echo "..."
    else
        echo "‚ùå Ollama test failed. Response: $response"
        exit 1
    fi
}

# Function to show status
show_status() {
    echo "üìä Ollama Status"
    echo "================"
    
    if docker ps | grep -q taskego-ollama; then
        echo "‚úÖ Container is running"
        echo "üÜî Container ID: $(docker ps --filter name=taskego-ollama --format '{{.ID}}')"
        echo "üåê Port: 11434"
        
        if curl -s http://localhost:11434/api/tags > /dev/null; then
            echo "üîå API is responding"
            echo "üìã Available models:"
            docker exec taskego-ollama ollama list 2>/dev/null || echo "No models loaded"
        else
            echo "‚ùå API is not responding"
        fi
    else
        echo "‚ùå Container is not running"
        echo "üí° Start it with: docker-compose up -d"
    fi
}

# Function to show logs
show_logs() {
    echo "üìã Ollama Logs"
    echo "=============="
    docker logs taskego-ollama --tail 50 -f
}

# Function to stop Ollama
stop_ollama() {
    echo "üõë Stopping Ollama..."
    docker-compose down
    echo "‚úÖ Ollama stopped"
}

# Function to show help
show_help() {
    echo "Usage: $0 [COMMAND]"
    echo ""
    echo "Commands:"
    echo "  local     Deploy Ollama locally"
    echo "  cloud     Deploy Ollama to cloud (requires REMOTE_HOST)"
    echo "  models    Pull recommended models"
    echo "  test      Test Ollama functionality"
    echo "  status    Show Ollama status"
    echo "  logs      Show Ollama logs"
    echo "  stop      Stop Ollama"
    echo "  help      Show this help message"
    echo ""
    echo "Examples:"
    echo "  $0 local                    # Deploy locally"
    echo "  $0 cloud                    # Deploy to cloud"
    echo "  export REMOTE_HOST=user@server.com && $0 cloud"
    echo "  $0 models                   # Pull models"
    echo "  $0 test                     # Test Ollama"
}

# Main script logic
case "${1:-help}" in
    "local")
        deploy_local
        ;;
    "cloud")
        deploy_cloud
        ;;
    "models")
        pull_models
        ;;
    "test")
        test_ollama
        ;;
    "status")
        show_status
        ;;
    "logs")
        show_logs
        ;;
    "stop")
        stop_ollama
        ;;
    "help"|*)
        show_help
        ;;
esac
